{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 060 Introduction to Scikit Learn\n",
    "\n",
    "> COM6018\n",
    "\n",
    "*Copyright &copy; 2023 Jon Barker, University of Sheffield. All rights reserved*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab class we will use the scikit-learn library to implement a k-Nearest Neighbours classifier and apply it to a simple classification problem. Before attempting this lab make sure that you have read the corresponding tutorial notes `060_Introducing_Scikit_Learn`\n",
    "\n",
    "\n",
    "## Background \n",
    "\n",
    "The classification task concerns the passive detection of land mines. The data set is described in the UCI Machine Learning Repository [here](https://archive.ics.uci.edu/dataset/763/land+mines-1). \n",
    "\n",
    "Landmines can be detected using a passive sensor similar to a metal detector. The detector is swept above the ground produces a voltage signal output. If the value is high then that indicates that there is possibly a mine present. This mine can then be carefully cleared. The problem is that there are other things that can cause a signal. Also the strength of the signal depends on how high the sensor is above the ground and the type of soil.\n",
    "\n",
    "Data is provide for 338 detections some of which are landmines and some of which are not. For each sample the following three measurements are provided:\n",
    "\n",
    "- Voltage (V) :Output voltage value from a magnetic sensor\n",
    "- High (H): The height of the sensor from the ground.\n",
    "- Soil Type (S): A value that corresponds to the amount of moisture in the soil\n",
    "\n",
    "The output label that we wish to predict is one of 5 classes:\n",
    "\n",
    "- 1: No Landmine \n",
    "- 2: Anti-Tank Mine\n",
    "- 3: Anti-Personnel Mine\n",
    "- 4: Booby trapped Anti-Tank Mine\n",
    "- 5: M14 Anti-Personnel Mine\n",
    "\n",
    "There are a roughly equal number of samples for each class. \n",
    "\n",
    "The primary task is to distinguish between class 1 (i..e no mine) and the other classes (i.e., some type of mine). This is a binary classification problem, i.e. a `detection` problem. But we will consider the multi-class problem of distinguishing between all 5 classes. If the detector could tell us which type of mine is likely to be present this would be useful information for the team that has to clear the mine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Loading the data\n",
    "\n",
    "The data is stored in an Excel spreadsheet filed called `data/Mine_Dataset.xls`. If you open the file you will see that it has several sheets. The one with the data is called `Normalized_Data`.\n",
    "\n",
    "We can read data from an Excel spreadsheet using the `read_excel` function from the `pandas` library. The function takes the name of the file as its first argument and the name of the sheet as its second argument. It returns a `DataFrame` object. Do this now in the cell below.\n",
    "\n",
    "(Remember that you will need to import the `pandas` library first.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE SOLUTION HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "assert len(df) == 338\n",
    "print('All tests passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, the features are stored in the columns labelled `V`, `H` and `S`. The labels are stored in the column labelled `M`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Visualize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now for a grid of scatter plots using the pairplot method from the seaborn library. Again, check the tutorial notes if you are not sure how to do this. Be sure to set the pairplot argument `hue` to the name of the column containing the labels.\n",
    "\n",
    "Implement this in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the scatter plot you can see that the data for the Soil (S) and Height (H) features are 'quantized', i.e., they only take a small number of values. The Soil feature can have values six possible value, (0.0, 0.2, ..., 1.0); The Height feature can have 12 values. All the features have been normalized to lie in the range [0,1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Generating a training and testing set\n",
    "\n",
    "We will now split the data into training and test sets. We will used 75% of the data for training and 25% for testing. We will do this using the train_test_split function from the sklearn.model_selection library. \n",
    "\n",
    "First of all we will need to separate the features (columns 'V', 'H' and 'S') and the labels into separate dataframes. We can do this using the `drop` method of the `DataFrame` object. The `drop` method takes a list of column names as its first argument and returns a new `DataFrame` object with the specified columns removed. The `drop` method does not modify the original `DataFrame` object.\n",
    "\n",
    "Set the train_test_split 'random_state' parameter to 1. This will ensure that the data is split in the same way each time the code is run.\n",
    "\n",
    "Write the code in the cell below and then check you code by running the test cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE SOLUTION HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "assert X_train.shape == (253,3)\n",
    "assert X_test.shape == (85, 3)\n",
    "assert y_train.shape == (253,)\n",
    "assert y_test.shape == (85,)\n",
    "assert X_train.iloc[0].V == 0.480361964603055\n",
    "assert y_train.iloc[0] == 3\n",
    "\n",
    "print('All tests passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Training a classifier\n",
    "\n",
    "We will now use a k-Nearest Neighbours classifier to classify the data. We will use the `KNeighborsClassifier` class from the `sklearn.neighbors` library. Initially use a value of k=1.\n",
    "\n",
    "In the cell below create a `KNeighborsClassifier` object and assign it to a variable called `model` and then call the fit function with the training data defined above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE SOLUTION HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert model.get_params()['n_neighbors'] == 1\n",
    "assert model._fit_X.shape == (253, 3)\n",
    "print('All tests passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model._fit_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Evaluate the model\n",
    "\n",
    "We will now use the model's predict method to predict the labels for the test data. Assign the predicted labels to a variable called `y_pred`. And then use the metrics.accuracy_score function to calculate the accuracy of the model. Store the accuracy in a variable called 'score' and print the value to the screen.\n",
    "\n",
    "Write the code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE SOLUTION HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "\n",
    "assert score == 0.29411764705882354\n",
    "print('All tests passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is very low, aorud 0.29, i.e. 29%. Given there are 5 classes we would expect to get around 20% accuracy by chance. The classifier is doing better than chance but it is still getting more classifications wrong than right.\n",
    "\n",
    "We will try to improve the score in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Tuning the model's hyperparameters\n",
    "\n",
    "The previous result has been achieved with a value of k=1. We will now try to improve the accuracy by trying different values of k. We will try values of k for all odd values in the range 1 to 21. \n",
    "\n",
    "This can be done by gathering together the steps above and placing them inside a for loop which iterates over the values of k. Inside the for loop add a print statement to print the value of k and the accuracy score.\n",
    "\n",
    "Write the code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should find that the best result comes from using a value of k = 5. This gives an accuracy of around 0.38, i.e. 38%. This is a significant improvement over the previous result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 - Analyzing the model performance\n",
    "\n",
    "We will now re-run the classifier using k = 5 and then use the confusion_matrix function to calculate the confusion matrix. Assign the result to a variable called `cm`.\n",
    "\n",
    "Then use ConfusionMatrixDisplay from sklearn.metrics to display the confusion matrix. You can set the `display_labels` parameter so that meaningful labels are displayed, e.g., `display_labels=['None', 'AP Mine', 'AT Mine', 'Booby', 'M14']`\n",
    "\n",
    "Refer to the module lecture notes if you are unsure how to compute and display confusion matrices.\n",
    "\n",
    "Write the code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE SOLUTION HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "assert cm.shape == (5, 5)\n",
    "assert cm[0, 0] == 11\n",
    "print('All tests passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the confusion matrix. You should see that there are a lot of correct classifications (i.e., numbers on the diagonal). However, there are also some cells off the diagonal which have high values. For example, the classifier is confusing AT mines, Booby-trapped AT Mines and AT M14 mines. This is not surprising as these two classes are very similar. More significantly, the M14 class is often been classified as 'None'. This is a serious problem as it means that the classifier is failing to detect M14 mines. (In the context of `detection` this is called a `false negative`.) We can also see some false positives, e.g., the classifier is classifying some 'None' samples as AT Mines. \n",
    "\n",
    "Consider a real-world application of this classifier. Which type of error would be more consequential? If the classifier was used to detect landmines in a field then the false positives would cause the field to be unnecessarily cleared. This would be expensive and time consuming. However, the false negatives are much more serious. If the classifier fails to detect a mine then it could result in the loss of life.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 - Evaluate the model using cross-validation\n",
    "\n",
    "In the previous evaluation we used 75% of the data for training and just 25% for testing. This meant that the test set was quite small and so the accuracy score and the values in the confusion matrix might not be very reliable.\n",
    "\n",
    "We will now use the leave-one-out training/test paradigm to evaluate the 3-nearest neighbour classifier more reliably.\n",
    "\n",
    "To do this you will need to use the 'cross_val_predict' method and the 'LeaveOneOut' class from 'sklearn.model_selection'. \n",
    "\n",
    "Run the leave-one-out cross-validation and then calculate the accuracy score and the confusion matrix. Display the confusion matrix using the ConfusionMatrixDisplay class. Also compute the accuracy (stored it in a variable called `accuracy`) and print it.\n",
    "\n",
    "An example of how to use these can be found in the tutorial notebook.\n",
    "\n",
    "Write the code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE SOLUTION HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "\n",
    "assert cm.shape == (5, 5)\n",
    "assert accuracy == 0.378698224852071\n",
    "print('All tests passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that the classifier is now making a significant number of false negative type errors, i.e. many of the mines are being classified as 'None'. \n",
    "\n",
    "Consider the task as a basic two class detection problem, i.e. with outputs 'Mine' or 'None', i.e. grouping all the mine classes into a single class. What is the accuracy of the classifier for this task? \n",
    "\n",
    "Write Python code to compute this accuracy in the cell below. Store it in a variable called `detection_accuracy.`\n",
    "\n",
    "(Hint: you can use the confusion matrix to calculate this.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE SOLUTION HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "assert detection_accuracy == 0.7840236686390533\n",
    "print('All tests passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should have found that the accuracy is about 78%.\n",
    "\n",
    "We can also work out the 'false negative rate', i.e. the percentage of detections where a land mine is missed. We want this to be very low.\n",
    "\n",
    "This is computed by summing the errors in the first column of the confusion matrix and dividing by the total number of detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_negatives = cm[1:, 0].sum()\n",
    "print('False negative rate = ', false_negatives / cm.sum() * 100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value is about 15%. This is far too high for a real-world application.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
